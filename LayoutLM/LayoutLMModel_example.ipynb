{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "634a254f",
   "metadata": {},
   "source": [
    "<https://huggingface.co/transformers/model_doc/layoutlm.html#layoutlmmodel>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5235589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-06 10:35:08.646484: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-06 10:35:08.646524: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMTokenizer, LayoutLMModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf445de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/layoutlm-base-uncased were not used when initializing LayoutLMModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing LayoutLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LayoutLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LayoutLMTokenizer.from_pretrained('microsoft/layoutlm-base-uncased')\n",
    "model = LayoutLMModel.from_pretrained('microsoft/layoutlm-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc230a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"Hello\", \"world\"]\n",
    "normalized_word_boxes = [637, 773, 693, 782], [698, 773, 733, 782]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda53f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokens = ['hello']\n",
      "word_tokens = ['world']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0],\n",
       " [637, 773, 693, 782],\n",
       " [698, 773, 733, 782],\n",
       " [1000, 1000, 1000, 1000]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_boxes = []\n",
    "for word, box in zip(words, normalized_word_boxes):\n",
    "    word_tokens = tokenizer.tokenize(word)\n",
    "    print(f\"word_tokens = {word_tokens }\")\n",
    "    token_boxes.extend([box] * len(word_tokens))\n",
    "# add bounding boxes of cls + sep tokens\n",
    "token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n",
    "token_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0001ab5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 2088,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer(' '.join(words), return_tensors=\"pt\")\n",
    "encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b94f2a2",
   "metadata": {},
   "source": [
    "I think `return_tensors` is to specify which type of tensor will be returned\n",
    "\n",
    "- `pt` signifies PyTorch\n",
    "- Let's try `tf` to see if it will return `tf.Tensor` from TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7fc8dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-06 10:35:20.336196: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-08-06 10:35:20.336256: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-08-06 10:35:20.336281: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (homography-x220t): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[ 101, 7592, 2088,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(' '.join(words), return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279e6fae",
   "metadata": {},
   "source": [
    "**(?)** We've got `input_ids` as `101, 7592, 2088,  102`. Do they mean\n",
    "\n",
    "- `101 -> <cls>`\n",
    "- `7592 -> hello`\n",
    "- `2088 -> world`\n",
    "- `102 -> <sep>`?\n",
    "\n",
    "Is there a dictionary for the correspondances?\n",
    "\n",
    "**(R)** Yes, just use the `decode()` method of the tokenizer instance. But do notice the difference of inputting arg as `int` and as list of `int`'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a190ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ C L S ]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02ad940c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [SEP]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([101, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f15906c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w o r l d'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(2088)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c473898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.decode(2088))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f769a482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello world [SEP]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([101, 7592, 2088,  102])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e940d6",
   "metadata": {},
   "source": [
    "### Other parameters to the instance method of `LayoutLMTokenizer`\n",
    "01. There is also a `padding` parameter for this instance method, which, when set to `\"max_length\"`, will pad to `512`\n",
    "02. `truncation`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "785cf1d8",
   "metadata": {},
   "source": [
    "tokenizer(' '.join(words), return_tensors=\"tf\", padding=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa7dbee",
   "metadata": {},
   "source": [
    "```python\n",
    "ValueError: 5 is not a valid PaddingStrategy, please select one of ['longest', 'max_length', 'do_not_pad']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b85c9887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
       "array([[ 101, 7592, 2088,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0]], dtype=int32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_padded = tokenizer(' '.join(words), return_tensors=\"tf\", padding=\"max_length\")\n",
    "max_padded[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "516d11d2",
   "metadata": {},
   "source": [
    "! {help(tokenizer.__call__)} | grep truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2daa521a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple pie apple pie apple pie '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"apple pie \" * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "366a31b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 6207,\n",
       " 11345,\n",
       " 102]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "many_apple_pie = tokenizer(\"apple pie \" * 512, truncation=True)[\"input_ids\"]\n",
    "many_apple_pie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aecf614d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(many_apple_pie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ca653d",
   "metadata": {},
   "source": [
    "Notice a few things\n",
    "\n",
    "01. mot only the output is being truncated to length `512`\n",
    "02. but also the first and last `input_ids` are `101` and `102`, which are presumably `<cls>` and `<sep>` tokens, resp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a5edd8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 102)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "many_apple_pie[0], many_apple_pie[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9f5a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b71d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoding[\"input_ids\"]\n",
    "attention_mask = encoding[\"attention_mask\"]\n",
    "token_type_ids = encoding[\"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8d36b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1377, -0.0217,  0.5168,  ..., -0.3557,  0.0057, -0.0747],\n",
       "         [ 0.2349, -0.0126,  0.2175,  ...,  0.3546,  0.4783, -0.5063],\n",
       "         [ 0.1883,  0.0240,  0.5338,  ..., -0.0384,  0.2696, -0.7188],\n",
       "         [ 0.1364, -0.0190,  0.5194,  ..., -0.3599,  0.0044, -0.0714]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.1216,  0.2186, -0.0275, -0.0761,  0.2830,  0.0359, -0.4294, -0.1931,\n",
       "          0.1522,  0.0605,  0.0462,  0.2257,  0.2945, -0.3746,  0.2547,  0.1569,\n",
       "          0.4103,  0.1967, -0.0695,  0.4194, -0.3590,  0.0327,  0.0486, -0.2101,\n",
       "         -0.1370,  0.1006,  0.2010, -0.0674,  0.3664, -0.0214,  0.3499, -0.3179,\n",
       "         -0.0975,  0.2563, -0.0059, -0.4568, -0.1184, -0.2711,  0.1816,  0.0836,\n",
       "         -0.1046, -0.2034,  0.3083,  0.3592,  0.0949,  0.2845,  0.2826, -0.1538,\n",
       "         -0.3005, -0.0784, -0.0088,  0.1495, -0.3902, -0.0309, -0.4107, -0.1594,\n",
       "         -0.0673, -0.1504,  0.2526, -0.2095,  0.2628, -0.0993, -0.0671, -0.4097,\n",
       "          0.1106,  0.0872, -0.0066,  0.3961,  0.1882, -0.0712,  0.0618, -0.0669,\n",
       "          0.3061,  0.1059, -0.1721, -0.2234, -0.2344, -0.3902,  0.0178, -0.2303,\n",
       "         -0.2888,  0.0556, -0.0386,  0.2796, -0.3577,  0.2267, -0.0649,  0.0641,\n",
       "         -0.2514, -0.0288, -0.3148,  0.1810, -0.1614,  0.0983,  0.3485, -0.1784,\n",
       "          0.1936, -0.2445,  0.4223, -0.2110, -0.0572, -0.0432,  0.1959,  0.0716,\n",
       "         -0.2695,  0.1059,  0.0137, -0.2499, -0.0137,  0.0735,  0.2341,  0.0656,\n",
       "          0.0216, -0.1685, -0.2367,  0.1262, -0.2100, -0.2661, -0.1280,  0.0798,\n",
       "          0.0083,  0.2370,  0.2479, -0.0032, -0.1446, -0.0188, -0.0048,  0.3713,\n",
       "         -0.2809,  0.1688,  0.2948,  0.2414, -0.2911, -0.1683,  0.2798,  0.1585,\n",
       "         -0.2700,  0.3369,  0.1279,  0.3178,  0.2720, -0.1206,  0.1426,  0.2376,\n",
       "         -0.2815, -0.2857, -0.1143, -0.2959, -0.1124, -0.0106, -0.1273,  0.1061,\n",
       "         -0.0470, -0.2744,  0.1675, -0.0745,  0.2525,  0.0720,  0.2908, -0.1257,\n",
       "          0.1845,  0.2613, -0.2327, -0.2281,  0.1064, -0.1993, -0.3650, -0.0162,\n",
       "          0.0014, -0.0265, -0.1120, -0.2275,  0.2928, -0.1206,  0.2993,  0.0027,\n",
       "          0.1849,  0.0560, -0.1047,  0.0269,  0.2621,  0.2170, -0.2819,  0.2107,\n",
       "          0.3280, -0.2428,  0.3633,  0.0280, -0.2448, -0.3208, -0.0858, -0.0194,\n",
       "         -0.2796,  0.1889,  0.2732,  0.2787,  0.0929,  0.3536,  0.1822,  0.2447,\n",
       "         -0.0090, -0.2497,  0.2126,  0.0192, -0.0082, -0.1397,  0.0039,  0.3731,\n",
       "         -0.0630, -0.2208, -0.1295,  0.1305,  0.3129,  0.0535, -0.0135, -0.3101,\n",
       "         -0.0358, -0.1537,  0.0777,  0.1123,  0.0314, -0.0768, -0.0212, -0.1778,\n",
       "          0.1194,  0.2171, -0.1952, -0.0422,  0.0122,  0.1138,  0.0483, -0.0803,\n",
       "         -0.0368, -0.5746,  0.0738,  0.2217, -0.0189, -0.1776,  0.0505, -0.0249,\n",
       "          0.3407,  0.1666,  0.2979,  0.1487, -0.1327, -0.0439,  0.1000,  0.2851,\n",
       "          0.2415, -0.2369,  0.0329,  0.1851, -0.0589,  0.2552,  0.2448, -0.2877,\n",
       "         -0.2851, -0.4232, -0.1610,  0.3933, -0.3264, -0.1154, -0.0124, -0.2001,\n",
       "         -0.0840,  0.1083, -0.2897, -0.5070,  0.0619, -0.1881, -0.0905,  0.1841,\n",
       "          0.1724, -0.1570, -0.1477,  0.0665,  0.0428, -0.3072, -0.3061, -0.1908,\n",
       "          0.1027, -0.0591,  0.1573,  0.0958,  0.0457,  0.0670,  0.1466, -0.3340,\n",
       "         -0.2159, -0.2087, -0.0014, -0.3031,  0.4887, -0.1304,  0.2321, -0.2971,\n",
       "          0.4521,  0.1612,  0.3777, -0.2254, -0.1408,  0.3398, -0.2080,  0.4016,\n",
       "          0.4054,  0.0172, -0.1244,  0.0295, -0.0368,  0.2157, -0.0937, -0.0008,\n",
       "          0.4269,  0.0771,  0.1254,  0.0733, -0.2049,  0.1547,  0.2742, -0.1980,\n",
       "         -0.1126, -0.1483,  0.2957,  0.0770, -0.2012, -0.1826, -0.0317, -0.1411,\n",
       "          0.1621,  0.0698, -0.1501,  0.1786, -0.0505, -0.1604, -0.2121,  0.1325,\n",
       "          0.1628,  0.1911,  0.4482,  0.0388,  0.3754,  0.0415,  0.2766, -0.1297,\n",
       "          0.1085, -0.3170,  0.1150, -0.0786,  0.0143, -0.1616,  0.0979, -0.1464,\n",
       "         -0.2584, -0.2792, -0.1996, -0.0772, -0.0632, -0.0305,  0.0995,  0.2541,\n",
       "          0.0280, -0.2611,  0.3783, -0.3689,  0.3429,  0.0893, -0.1100,  0.1607,\n",
       "          0.1118, -0.0429, -0.1696, -0.1012,  0.4177, -0.2821, -0.2342,  0.1692,\n",
       "         -0.0725,  0.0086, -0.3208,  0.1541, -0.1356, -0.2386, -0.1370, -0.1702,\n",
       "          0.1265, -0.3027, -0.0213, -0.1540,  0.2850, -0.1476, -0.2863, -0.0899,\n",
       "         -0.2963,  0.0906,  0.0043,  0.0824,  0.0108, -0.3146, -0.1663, -0.0563,\n",
       "          0.2186, -0.1274,  0.3455,  0.2291, -0.2605, -0.1007, -0.3350,  0.4067,\n",
       "         -0.1156, -0.1623, -0.0021, -0.1077,  0.4045, -0.0578, -0.0267,  0.2041,\n",
       "          0.4488,  0.2856,  0.3627, -0.0935, -0.1492,  0.1100, -0.1899, -0.2082,\n",
       "         -0.3225,  0.1344, -0.2474,  0.1117, -0.0940, -0.1836, -0.1531, -0.0840,\n",
       "         -0.3958,  0.1657, -0.2378,  0.0079, -0.2147, -0.3619,  0.1376,  0.0765,\n",
       "          0.2750,  0.0225,  0.2521, -0.1359, -0.2900, -0.3091,  0.1949,  0.2407,\n",
       "         -0.3038, -0.2240, -0.0554, -0.1748,  0.0276,  0.1510, -0.3530,  0.0730,\n",
       "          0.2450, -0.1016,  0.0550,  0.3162, -0.0574,  0.2471,  0.0602,  0.0493,\n",
       "          0.3113, -0.1734,  0.1840,  0.1860,  0.0604, -0.2547, -0.1941,  0.3447,\n",
       "         -0.0645, -0.3380,  0.0937,  0.1678,  0.1865,  0.1042,  0.3250, -0.0414,\n",
       "          0.1583, -0.0789,  0.5113, -0.2181,  0.4867,  0.1625,  0.3399,  0.0764,\n",
       "         -0.1985,  0.0885,  0.0063, -0.0441,  0.1119, -0.1216, -0.2465, -0.0065,\n",
       "         -0.0789,  0.0129,  0.3187,  0.1198,  0.2145, -0.2832, -0.0249, -0.1855,\n",
       "         -0.2358,  0.0464, -0.1371, -0.1424,  0.0184, -0.1996, -0.0929, -0.3323,\n",
       "          0.2529, -0.1292,  0.1120,  0.0274, -0.0945, -0.3415,  0.3856, -0.0078,\n",
       "         -0.2815, -0.0187, -0.0553, -0.1503,  0.2277,  0.2722,  0.2933, -0.0083,\n",
       "         -0.1020, -0.6372,  0.1910, -0.1402, -0.0824, -0.0094,  0.0492, -0.1592,\n",
       "         -0.1400, -0.4047,  0.3340,  0.1343, -0.0329, -0.2422,  0.0615, -0.0682,\n",
       "         -0.2528, -0.4017,  0.0749,  0.0430, -0.3486,  0.1145, -0.3850,  0.0776,\n",
       "         -0.0420, -0.2598,  0.1199, -0.0180, -0.0631, -0.1109, -0.2550,  0.1805,\n",
       "         -0.1082, -0.0333,  0.0846,  0.0070, -0.0662, -0.1870,  0.1203,  0.0440,\n",
       "         -0.0482,  0.2048, -0.0540,  0.1611,  0.0667,  0.1123, -0.2844,  0.4749,\n",
       "          0.3578, -0.1447,  0.0017,  0.2275, -0.1317, -0.1825, -0.1028,  0.0877,\n",
       "         -0.0794,  0.2686, -0.0098,  0.2689, -0.1193, -0.1455,  0.1009, -0.1900,\n",
       "          0.2842, -0.3944,  0.1290, -0.4094, -0.1450,  0.2094, -0.2094, -0.1938,\n",
       "          0.1204, -0.3190,  0.2088, -0.4502, -0.2519,  0.1470,  0.0382, -0.0976,\n",
       "         -0.0875,  0.1495, -0.3033, -0.0420, -0.3409, -0.2293, -0.0103,  0.1146,\n",
       "         -0.1332, -0.1127, -0.2381,  0.1637, -0.0798,  0.2660, -0.2430,  0.0732,\n",
       "          0.4350, -0.1004, -0.3110, -0.2580, -0.2063, -0.2048, -0.2307,  0.0969,\n",
       "         -0.1448,  0.2164, -0.3635, -0.3319, -0.2023,  0.4636,  0.1560,  0.3377,\n",
       "         -0.1634, -0.2451,  0.2631, -0.1153, -0.1843, -0.0416, -0.2146, -0.4630,\n",
       "         -0.1106, -0.2487, -0.1051, -0.2854,  0.4339, -0.0188, -0.1167, -0.0156,\n",
       "          0.1074, -0.0974,  0.0702,  0.0774, -0.2599,  0.0415, -0.0612, -0.1530,\n",
       "          0.1504,  0.2129,  0.1440, -0.0392, -0.0509,  0.1177, -0.0432,  0.1021,\n",
       "         -0.1051, -0.0012, -0.4417, -0.4070, -0.0490, -0.0500,  0.1891, -0.0619,\n",
       "         -0.0913,  0.2019,  0.1047, -0.1563,  0.0426,  0.1567, -0.2560, -0.2326,\n",
       "          0.0177,  0.3146, -0.0267,  0.3790,  0.0550,  0.0586,  0.2373,  0.1431,\n",
       "          0.3070, -0.3694, -0.0204, -0.0110,  0.2101, -0.0539,  0.0670,  0.0384,\n",
       "         -0.1364,  0.2364,  0.2178, -0.0334,  0.1464,  0.1568,  0.0682,  0.1811,\n",
       "          0.1379,  0.0627, -0.2535,  0.3301,  0.0330,  0.1523,  0.0479, -0.1490,\n",
       "          0.3638, -0.2420, -0.4063, -0.1263,  0.3311,  0.0244, -0.1681, -0.2197,\n",
       "         -0.1052,  0.2932,  0.0026,  0.2607,  0.1503, -0.2678,  0.0998,  0.3252,\n",
       "          0.2464, -0.2572,  0.1930,  0.2087, -0.0308, -0.2809,  0.3132, -0.2073,\n",
       "          0.1860,  0.2563, -0.1654,  0.1549,  0.0318,  0.1706,  0.1901,  0.1774,\n",
       "          0.3311,  0.1281,  0.1227,  0.0832,  0.0542, -0.0766,  0.2541,  0.0600,\n",
       "          0.3581,  0.2928, -0.1784,  0.3656,  0.1572, -0.1989,  0.0097,  0.0723]],\n",
       "       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox = torch.tensor([token_boxes])\n",
    "outputs = model(input_ids=input_ids,\n",
    "                bbox=bbox,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    ")\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fcdcde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "509a07ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attentions',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'cross_attentions',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'hidden_states',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'last_hidden_state',\n",
       " 'move_to_end',\n",
       " 'past_key_values',\n",
       " 'pooler_output',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'to_tuple',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in dir(outputs) if not s.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8b39534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 768]), torch.Size([1, 4, 768]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output.shape, outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1edd6",
   "metadata": {},
   "source": [
    "Note that\n",
    "\n",
    "- we have called `model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids)`, treating the `LayoutLMModel` instance as a function. This is probably via the method `__call__()` of the class.\n",
    "- It is said in the doc that the above way of calling the model is preferred to `model.forward()` because\n",
    "  - calling the instance will handle the pre and post processing\n",
    "  - whereas calling its `forward()` method won't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479c1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
